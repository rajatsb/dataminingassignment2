{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "121101835_CS3033/CS6405_Data Mining _Second_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS3033/CS6405 - Data Mining - Second Assignment\n",
        "\n",
        "### Submission\n",
        "\n",
        "This assignment is **due on 06/04/22 at 23:59**. You should submit a single .ipnyb file with your python code and analysis electronically via Canvas.\n",
        "Please note that this assignment will account for 25 Marks of your module grade.\n",
        "\n",
        "### Declaration\n",
        "\n",
        "By submitting this assignment. I agree to the following:\n",
        "\n",
        "<font color=\"red\">“I have read and understand the UCC academic policy on plagiarism, and agree to the requirements set out thereby in relation to plagiarism and referencing. I confirm that I have referenced and acknowledged properly all sources used in the preparation of this assignment.\n",
        "I declare that this assignment is entirely my own work based on my personal study. I further declare that I have not engaged the services of another to either assist me in, or complete this assignment”</font>\n",
        "\n",
        "### Objective\n",
        "\n",
        "The Boolean satisfiability (SAT) problem consists in determining whether a Boolean formula F is satisfiable or not. F is represented by a pair (X, C), where X is a set of Boolean variables and C is a set of clauses in Conjunctive Normal Form (CNF). Each clause is a disjunction of literals (a variable or its negation). This problem is one of the most widely studied combinatorial problems in computer science. It is the classic NP-complete problem. Over the past number of decades, a significant amount of research work has focused on solving SAT problems with both complete and incomplete solvers.\n",
        "\n",
        "Recent advances in supervised learning have provided powerful techniques for classifying problems. In this project, we see the SAT problem as a classification problem. Given a Boolean formula (represented by a vector of features), we are asked to predict if it is satisfiable or not.\n",
        "\n",
        "In this project, we represent SAT problems with a vector of 327 features with general information about the problem, e.g., number of variables, number of clauses, fraction of horn clauses in the problem, etc. There is no need to understand the features to be able to complete the assignment.\n",
        "\n",
        "The dataset is available at:\n",
        "https://github.com/andvise/DataAnalyticsDatasets/blob/main/dm_assignment2/sat_dataset_train.csv\n",
        "\n",
        "This is original unpublished data."
      ],
      "metadata": {
        "id": "8WfrCFmLHxYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "Oav9G1WSJ1nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.formats.info import DataFrameInfo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"https://github.com/andvise/DataAnalyticsDatasets/blob/6d5738101d173b97c565f143f945dedb9c42a400/dm_assignment2/sat_dataset_train.csv?raw=true\")\n",
        "\n",
        "value = df.quantile(0.98)\n",
        "\n",
        "df=df.round(decimals = 2)\n",
        "#df = df.replace(np.inf, value)\n",
        "#df = df.replace(-np.inf, value)\n",
        "df.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "\n",
        "df.fillna(0,inplace=True)\n",
        "#df = df.drop(['saps_EstACL_Mean'], axis=1)\n",
        "\n",
        "df\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "DE0kM0QsJ1En",
        "outputId": "e594883a-5ccb-4c38-c1e6-633f0524789b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        c    v  clauses_vars_ratio  vars_clauses_ratio  vcg_var_mean  \\\n",
              "0     420   10               42.00                0.02          0.60   \n",
              "1     230   20               11.50                0.09          0.14   \n",
              "2     240   16               15.00                0.07          0.30   \n",
              "3     424   30               14.13                0.07          0.23   \n",
              "4     162   19                8.53                0.12          0.14   \n",
              "...   ...  ...                 ...                 ...           ...   \n",
              "1924  910   50               18.20                0.05          0.05   \n",
              "1925  440   30               14.67                0.07          0.23   \n",
              "1926  372   28               13.29                0.08          0.10   \n",
              "1927  821  181                4.54                0.22          0.02   \n",
              "1928  775   90                8.61                0.12          0.03   \n",
              "\n",
              "      vcg_var_coeff  vcg_var_min  vcg_var_max  vcg_var_entropy  \\\n",
              "0              0.00         0.60         0.60             0.00   \n",
              "1              0.09         0.12         0.16             2.18   \n",
              "2              0.00         0.30         0.30             0.00   \n",
              "3              0.49         0.06         0.45             2.22   \n",
              "4              0.12         0.11         0.19             1.94   \n",
              "...             ...          ...          ...              ...   \n",
              "1924           0.20         0.02         0.07             3.20   \n",
              "1925           0.46         0.05         0.44             2.24   \n",
              "1926           0.08         0.08         0.11             2.01   \n",
              "1927           0.03         0.02         0.02             0.70   \n",
              "1928           0.02         0.03         0.03             0.64   \n",
              "\n",
              "      vcg_clause_mean  ...  rwh_0_max  rwh_1_mean  rwh_1_coeff  rwh_1_min  \\\n",
              "0                0.60  ...    78750.0        0.00          0.0       0.00   \n",
              "1                0.14  ...  6646875.0    17433.72          1.0       0.00   \n",
              "2                0.30  ...   500000.0     1525.88          0.0    1525.88   \n",
              "3                0.23  ...    87500.0        0.00          1.0       0.00   \n",
              "4                0.14  ...  5859400.0    16591.49          1.0       0.00   \n",
              "...               ...  ...        ...         ...          ...        ...   \n",
              "1924             0.05  ...  7031250.0     5937.50          1.0       0.00   \n",
              "1925             0.23  ...    29000.0        0.00          1.0       0.00   \n",
              "1926             0.10  ...  6640650.0    11547.13          1.0       0.00   \n",
              "1927             0.02  ...  3515625.0      107.31          1.0       0.00   \n",
              "1928             0.03  ...  1640625.0       34.89          1.0       0.02   \n",
              "\n",
              "      rwh_1_max  rwh_2_mean  rwh_2_coeff  rwh_2_min  rwh_2_max  target  \n",
              "0          0.00        0.00          0.0       0.00       0.00       1  \n",
              "1      34867.44    17277.21          1.0       0.00   34554.42       0  \n",
              "2       1525.88     1525.88          0.0    1525.88    1525.88       1  \n",
              "3          0.00        0.00          1.0       0.00       0.00       0  \n",
              "4      33182.99    16659.03          1.0       0.00   33318.07       1  \n",
              "...         ...         ...          ...        ...        ...     ...  \n",
              "1924   11874.99     5759.32          1.0       0.00   11518.64       1  \n",
              "1925       0.00        0.00          1.0       0.00       0.00       0  \n",
              "1926   23094.26    11366.08          1.0       0.00   22732.16       0  \n",
              "1927     214.62      107.31          1.0       0.00     214.62       1  \n",
              "1928      69.76       48.23          1.0       0.04      96.42       0  \n",
              "\n",
              "[1929 rows x 328 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e00eb4e7-15e3-44ec-a10b-d03189a48088\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>v</th>\n",
              "      <th>clauses_vars_ratio</th>\n",
              "      <th>vars_clauses_ratio</th>\n",
              "      <th>vcg_var_mean</th>\n",
              "      <th>vcg_var_coeff</th>\n",
              "      <th>vcg_var_min</th>\n",
              "      <th>vcg_var_max</th>\n",
              "      <th>vcg_var_entropy</th>\n",
              "      <th>vcg_clause_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>rwh_0_max</th>\n",
              "      <th>rwh_1_mean</th>\n",
              "      <th>rwh_1_coeff</th>\n",
              "      <th>rwh_1_min</th>\n",
              "      <th>rwh_1_max</th>\n",
              "      <th>rwh_2_mean</th>\n",
              "      <th>rwh_2_coeff</th>\n",
              "      <th>rwh_2_min</th>\n",
              "      <th>rwh_2_max</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>420</td>\n",
              "      <td>10</td>\n",
              "      <td>42.00</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.60</td>\n",
              "      <td>...</td>\n",
              "      <td>78750.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>230</td>\n",
              "      <td>20</td>\n",
              "      <td>11.50</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.16</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.14</td>\n",
              "      <td>...</td>\n",
              "      <td>6646875.0</td>\n",
              "      <td>17433.72</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>34867.44</td>\n",
              "      <td>17277.21</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>34554.42</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>240</td>\n",
              "      <td>16</td>\n",
              "      <td>15.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.30</td>\n",
              "      <td>...</td>\n",
              "      <td>500000.0</td>\n",
              "      <td>1525.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1525.88</td>\n",
              "      <td>1525.88</td>\n",
              "      <td>1525.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1525.88</td>\n",
              "      <td>1525.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>424</td>\n",
              "      <td>30</td>\n",
              "      <td>14.13</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.45</td>\n",
              "      <td>2.22</td>\n",
              "      <td>0.23</td>\n",
              "      <td>...</td>\n",
              "      <td>87500.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>162</td>\n",
              "      <td>19</td>\n",
              "      <td>8.53</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.94</td>\n",
              "      <td>0.14</td>\n",
              "      <td>...</td>\n",
              "      <td>5859400.0</td>\n",
              "      <td>16591.49</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>33182.99</td>\n",
              "      <td>16659.03</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>33318.07</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1924</th>\n",
              "      <td>910</td>\n",
              "      <td>50</td>\n",
              "      <td>18.20</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.07</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.05</td>\n",
              "      <td>...</td>\n",
              "      <td>7031250.0</td>\n",
              "      <td>5937.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>11874.99</td>\n",
              "      <td>5759.32</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>11518.64</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1925</th>\n",
              "      <td>440</td>\n",
              "      <td>30</td>\n",
              "      <td>14.67</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.44</td>\n",
              "      <td>2.24</td>\n",
              "      <td>0.23</td>\n",
              "      <td>...</td>\n",
              "      <td>29000.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1926</th>\n",
              "      <td>372</td>\n",
              "      <td>28</td>\n",
              "      <td>13.29</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>2.01</td>\n",
              "      <td>0.10</td>\n",
              "      <td>...</td>\n",
              "      <td>6640650.0</td>\n",
              "      <td>11547.13</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23094.26</td>\n",
              "      <td>11366.08</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>22732.16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1927</th>\n",
              "      <td>821</td>\n",
              "      <td>181</td>\n",
              "      <td>4.54</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.02</td>\n",
              "      <td>...</td>\n",
              "      <td>3515625.0</td>\n",
              "      <td>107.31</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>214.62</td>\n",
              "      <td>107.31</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>214.62</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1928</th>\n",
              "      <td>775</td>\n",
              "      <td>90</td>\n",
              "      <td>8.61</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.03</td>\n",
              "      <td>...</td>\n",
              "      <td>1640625.0</td>\n",
              "      <td>34.89</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>69.76</td>\n",
              "      <td>48.23</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.04</td>\n",
              "      <td>96.42</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1929 rows × 328 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e00eb4e7-15e3-44ec-a10b-d03189a48088')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e00eb4e7-15e3-44ec-a10b-d03189a48088 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e00eb4e7-15e3-44ec-a10b-d03189a48088');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "WOqpQTS-K8EN",
        "outputId": "5b4c7602-769c-4622-b853-6a24886abe75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        c    v  clauses_vars_ratio  vars_clauses_ratio  vcg_var_mean  \\\n",
              "0     420   10               42.00                0.02          0.60   \n",
              "1     230   20               11.50                0.09          0.14   \n",
              "2     240   16               15.00                0.07          0.30   \n",
              "3     424   30               14.13                0.07          0.23   \n",
              "4     162   19                8.53                0.12          0.14   \n",
              "...   ...  ...                 ...                 ...           ...   \n",
              "1924  910   50               18.20                0.05          0.05   \n",
              "1925  440   30               14.67                0.07          0.23   \n",
              "1926  372   28               13.29                0.08          0.10   \n",
              "1927  821  181                4.54                0.22          0.02   \n",
              "1928  775   90                8.61                0.12          0.03   \n",
              "\n",
              "      vcg_var_coeff  vcg_var_min  vcg_var_max  vcg_var_entropy  \\\n",
              "0              0.00         0.60         0.60             0.00   \n",
              "1              0.09         0.12         0.16             2.18   \n",
              "2              0.00         0.30         0.30             0.00   \n",
              "3              0.49         0.06         0.45             2.22   \n",
              "4              0.12         0.11         0.19             1.94   \n",
              "...             ...          ...          ...              ...   \n",
              "1924           0.20         0.02         0.07             3.20   \n",
              "1925           0.46         0.05         0.44             2.24   \n",
              "1926           0.08         0.08         0.11             2.01   \n",
              "1927           0.03         0.02         0.02             0.70   \n",
              "1928           0.02         0.03         0.03             0.64   \n",
              "\n",
              "      vcg_clause_mean  ...  rwh_0_max  rwh_1_mean  rwh_1_coeff  rwh_1_min  \\\n",
              "0                0.60  ...    78750.0        0.00          0.0       0.00   \n",
              "1                0.14  ...  6646875.0    17433.72          1.0       0.00   \n",
              "2                0.30  ...   500000.0     1525.88          0.0    1525.88   \n",
              "3                0.23  ...    87500.0        0.00          1.0       0.00   \n",
              "4                0.14  ...  5859400.0    16591.49          1.0       0.00   \n",
              "...               ...  ...        ...         ...          ...        ...   \n",
              "1924             0.05  ...  7031250.0     5937.50          1.0       0.00   \n",
              "1925             0.23  ...    29000.0        0.00          1.0       0.00   \n",
              "1926             0.10  ...  6640650.0    11547.13          1.0       0.00   \n",
              "1927             0.02  ...  3515625.0      107.31          1.0       0.00   \n",
              "1928             0.03  ...  1640625.0       34.89          1.0       0.02   \n",
              "\n",
              "      rwh_1_max  rwh_2_mean  rwh_2_coeff  rwh_2_min  rwh_2_max  target  \n",
              "0          0.00        0.00          0.0       0.00       0.00       1  \n",
              "1      34867.44    17277.21          1.0       0.00   34554.42       0  \n",
              "2       1525.88     1525.88          0.0    1525.88    1525.88       1  \n",
              "3          0.00        0.00          1.0       0.00       0.00       0  \n",
              "4      33182.99    16659.03          1.0       0.00   33318.07       1  \n",
              "...         ...         ...          ...        ...        ...     ...  \n",
              "1924   11874.99     5759.32          1.0       0.00   11518.64       1  \n",
              "1925       0.00        0.00          1.0       0.00       0.00       0  \n",
              "1926   23094.26    11366.08          1.0       0.00   22732.16       0  \n",
              "1927     214.62      107.31          1.0       0.00     214.62       1  \n",
              "1928      69.76       48.23          1.0       0.04      96.42       0  \n",
              "\n",
              "[1929 rows x 328 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99e3d025-ad0e-42aa-8ec1-dc71bd56f26c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>v</th>\n",
              "      <th>clauses_vars_ratio</th>\n",
              "      <th>vars_clauses_ratio</th>\n",
              "      <th>vcg_var_mean</th>\n",
              "      <th>vcg_var_coeff</th>\n",
              "      <th>vcg_var_min</th>\n",
              "      <th>vcg_var_max</th>\n",
              "      <th>vcg_var_entropy</th>\n",
              "      <th>vcg_clause_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>rwh_0_max</th>\n",
              "      <th>rwh_1_mean</th>\n",
              "      <th>rwh_1_coeff</th>\n",
              "      <th>rwh_1_min</th>\n",
              "      <th>rwh_1_max</th>\n",
              "      <th>rwh_2_mean</th>\n",
              "      <th>rwh_2_coeff</th>\n",
              "      <th>rwh_2_min</th>\n",
              "      <th>rwh_2_max</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>420</td>\n",
              "      <td>10</td>\n",
              "      <td>42.00</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.60</td>\n",
              "      <td>...</td>\n",
              "      <td>78750.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>230</td>\n",
              "      <td>20</td>\n",
              "      <td>11.50</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.16</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.14</td>\n",
              "      <td>...</td>\n",
              "      <td>6646875.0</td>\n",
              "      <td>17433.72</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>34867.44</td>\n",
              "      <td>17277.21</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>34554.42</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>240</td>\n",
              "      <td>16</td>\n",
              "      <td>15.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.30</td>\n",
              "      <td>...</td>\n",
              "      <td>500000.0</td>\n",
              "      <td>1525.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1525.88</td>\n",
              "      <td>1525.88</td>\n",
              "      <td>1525.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1525.88</td>\n",
              "      <td>1525.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>424</td>\n",
              "      <td>30</td>\n",
              "      <td>14.13</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.45</td>\n",
              "      <td>2.22</td>\n",
              "      <td>0.23</td>\n",
              "      <td>...</td>\n",
              "      <td>87500.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>162</td>\n",
              "      <td>19</td>\n",
              "      <td>8.53</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.94</td>\n",
              "      <td>0.14</td>\n",
              "      <td>...</td>\n",
              "      <td>5859400.0</td>\n",
              "      <td>16591.49</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>33182.99</td>\n",
              "      <td>16659.03</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>33318.07</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1924</th>\n",
              "      <td>910</td>\n",
              "      <td>50</td>\n",
              "      <td>18.20</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.07</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.05</td>\n",
              "      <td>...</td>\n",
              "      <td>7031250.0</td>\n",
              "      <td>5937.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>11874.99</td>\n",
              "      <td>5759.32</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>11518.64</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1925</th>\n",
              "      <td>440</td>\n",
              "      <td>30</td>\n",
              "      <td>14.67</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.44</td>\n",
              "      <td>2.24</td>\n",
              "      <td>0.23</td>\n",
              "      <td>...</td>\n",
              "      <td>29000.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1926</th>\n",
              "      <td>372</td>\n",
              "      <td>28</td>\n",
              "      <td>13.29</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>2.01</td>\n",
              "      <td>0.10</td>\n",
              "      <td>...</td>\n",
              "      <td>6640650.0</td>\n",
              "      <td>11547.13</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23094.26</td>\n",
              "      <td>11366.08</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>22732.16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1927</th>\n",
              "      <td>821</td>\n",
              "      <td>181</td>\n",
              "      <td>4.54</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.02</td>\n",
              "      <td>...</td>\n",
              "      <td>3515625.0</td>\n",
              "      <td>107.31</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>214.62</td>\n",
              "      <td>107.31</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>214.62</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1928</th>\n",
              "      <td>775</td>\n",
              "      <td>90</td>\n",
              "      <td>8.61</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.03</td>\n",
              "      <td>...</td>\n",
              "      <td>1640625.0</td>\n",
              "      <td>34.89</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>69.76</td>\n",
              "      <td>48.23</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.04</td>\n",
              "      <td>96.42</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1929 rows × 328 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99e3d025-ad0e-42aa-8ec1-dc71bd56f26c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-99e3d025-ad0e-42aa-8ec1-dc71bd56f26c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-99e3d025-ad0e-42aa-8ec1-dc71bd56f26c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['target'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8MCvTYTKw4Q",
        "outputId": "5991f0f4-7ee4-4df6-9087-014b67636b06"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    976\n",
              "0    953\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "df1= df\n",
        "y= df['target']\n",
        "x = df1.iloc[: , :-1]\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.70,test_size=0.30,random_state=1)\n",
        "\n",
        "print(y_test)\n"
      ],
      "metadata": {
        "id": "IEjqR2xLKjec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d778fe0d-126a-4217-9ca1-7f8c06225571"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48      0\n",
            "1283    0\n",
            "1163    1\n",
            "941     1\n",
            "309     1\n",
            "       ..\n",
            "1561    1\n",
            "142     0\n",
            "1613    0\n",
            "1057    1\n",
            "1138    0\n",
            "Name: target, Length: 579, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks\n",
        "\n",
        "## Basic models and evaluation (5 Marks)\n",
        "\n",
        "Using Scikit-learn, train and evaluate K-NN and decision tree classifiers using 70% of the dataset from training and 30% for testing. For this part of the project, we are not interested in optimising the parameters; we just want to get an idea of the dataset. Compare the results of both classifiers."
      ],
      "metadata": {
        "id": "MTvkBPQvITf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "standard_scaler = StandardScaler()\n",
        "standard_scaler.fit(x_train)\n",
        "x_train_val=standard_scaler.transform(x_train)\n",
        "x_test_val=standard_scaler.transform(x_test)\n",
        "\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors = 1,  metric='manhattan')\n",
        "knn.fit(x_train,y_train)\n",
        "\n",
        "y_pred=knn.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix_main = confusion_matrix(y_test,y_pred).ravel().tolist()\n",
        "print(confusion_matrix_main)\n",
        "accuracy= (confusion_matrix_main[0]+confusion_matrix_main[3]) / (confusion_matrix_main[0]+confusion_matrix_main[1]+confusion_matrix_main[2]+confusion_matrix_main[3])\n",
        "print(\"Accuracy for KNN: \",accuracy)\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "decisiontree = tree.DecisionTreeClassifier(criterion='entropy')\n",
        "decisiontree.fit(x_train,y_train)\n",
        "\n",
        "y_predict_decision_tree = decisiontree.predict(x_test)\n",
        "\n",
        "confusion_matrix_main2 = confusion_matrix(y_test,y_predict_decision_tree).ravel().tolist()\n",
        "print(confusion_matrix_main2)\n",
        "accuracy2= (confusion_matrix_main2[0]+confusion_matrix_main2[3]) / (confusion_matrix_main2[0]+confusion_matrix_main2[1]+confusion_matrix_main2[2]+confusion_matrix_main2[3])\n",
        "print(\"Accuracy For Decision Tree: \",accuracy2)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print(\"Roc Score\",roc_auc_score(y_pred,y_test))\n",
        "\n",
        "accuracyknn = []\n",
        "\n",
        "# Calculating error for K values between 1 and 40\n",
        "for i in range(1, 15):\n",
        "    knn = KNeighborsClassifier(n_neighbors=i)\n",
        "    knn.fit(x_train, y_train)\n",
        "    pred_i = knn.predict(x_test)\n",
        "    accuracyknn.append(np.mean(pred_i == y_test))\n",
        "\n",
        "print(accuracyknn)\n",
        "\n",
        "#lets plot some summary for accuracy of K values between 1 to 15 and check the optimal values.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.plot(range(1,15),accuracyknn,color='blue',linestyle='dotted',marker='o',markerfacecolor='pink',markersize='12')\n",
        "\n",
        "plt.title(\"Accuracy K values\")\n",
        "plt.xlabel(\"K value\")\n",
        "plt.ylabel(\"Accuracy\")"
      ],
      "metadata": {
        "id": "Zl0VXO0YH1nG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "86eaf423-61a5-4f15-ee80-bb48abcbc2eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[236, 52, 51, 240]\n",
            "Accuracy for KNN:  0.8221070811744386\n",
            "[287, 1, 5, 286]\n",
            "Accuracy For Decision Tree:  0.9896373056994818\n",
            "Roc Score 0.8221087298935612\n",
            "[0.8186528497409327, 0.772020725388601, 0.7875647668393783, 0.7737478411053541, 0.7616580310880829, 0.7530224525043178, 0.7564766839378239, 0.7633851468048359, 0.7409326424870466, 0.7495682210708118, 0.7271157167530224, 0.7409326424870466, 0.7512953367875648, 0.7322970639032815]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Accuracy')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robust evaluation (10 Marks)\n",
        "\n",
        "In this section, we are interested in more rigorous techniques by implementing more sophisticated methods, for instance:\n",
        "* Hold-out and cross-validation.\n",
        "* Hyper-parameter tuning.\n",
        "* Feature reduction.\n",
        "* Feature normalisation.\n",
        "\n",
        "Your report should provide concrete information of your reasoning; everything should be well-explained.\n",
        "\n",
        "Do not get stressed if the things you try do not improve the accuracy. The key to geting good marks is to show that you evaluated different methods and that you correctly selected the configuration."
      ],
      "metadata": {
        "id": "zADpr0f8IcGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import neighbors\n",
        "from sklearn import metrics\n",
        "# YOUR CODE HERE\n",
        "#Hold Out and Cross Validation\n",
        "#Here K fold cross validation is used\n",
        "from sklearn.model_selection import cross_val_score\n",
        "#knn2=KNeighborsClassifier()\n",
        "#print(knn2)\n",
        "knn_score = cross_val_score(knn,x_train,y_train,cv=10)\n",
        "decision_tree_cross_validation_score = cross_val_score(decisiontree,x,y,cv=10)\n",
        "\n",
        "print(knn_score)\n",
        "print(decision_tree_cross_validation_score)\n",
        "print(\"Mean Cross Validation Score For KNN with k folds: \",knn_score.mean())\n",
        "print(\"Mean Cross Validation Score For Decision Tree with k folds: \",decision_tree_cross_validation_score.mean())\n",
        "\n",
        "#For Hyperparameter tuning we are considering grid search cv\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#hyperparameters require for knn'\n",
        "#The hyperparameter for knn is choosen as number of k values and distance metric in sklearn the distance metric is termed as p and if p=1 the distance is manhattan, if p=2, the distance metric is euclidian\n",
        "\n",
        "n_neighbors = list(range(1,20))\n",
        "distance_metric=[1,2]\n",
        "\n",
        "hypermarameter = dict(n_neighbors = n_neighbors,p=distance_metric)\n",
        "\n",
        "knn_check= KNeighborsClassifier()\n",
        "#reference: https://medium.datadriveninvestor.com/k-nearest-neighbors-in-python-hyperparameters-tuning-716734bc557f\n",
        "\n",
        "gridcv= GridSearchCV(knn,hypermarameter,cv=10)\n",
        "best_model = gridcv.fit(x_train,y_train)\n",
        "print(\"Best value of K:\",best_model.best_estimator_.get_params()['n_neighbors'])\n",
        "\n",
        "print(\"Best Distance Metric:\",best_model.best_estimator_.get_params()['p'])\n",
        "knn_prediction = gridcv.predict(x_test)\n",
        "print(\"Accuracy for knn Tree After Hyperparameter Tuning : \",metrics.accuracy_score(y_test,knn_prediction))\n",
        "\n",
        "\n",
        "\n",
        "#hyperparameter tuning for decision tree using gridcv\n",
        "#here we will using two hyperparameters. One is criterion metric that has two values gini index and another one is information Gain\n",
        "# and another hyperparameter will be the max depth.\n",
        "\n",
        "criterion = ['gini','entropy']\n",
        "max_depth=list(range(1,15))\n",
        "\n",
        "hyperparameter_decision_tree = dict(criterion=criterion,max_depth=max_depth)\n",
        "\n",
        "gridcvdecisiontree = GridSearchCV(decisiontree,hyperparameter_decision_tree,cv=10)\n",
        "best_tree = gridcvdecisiontree.fit(x_train,y_train)\n",
        "\n",
        "dt_prediction = gridcvdecisiontree.predict(x_test)\n",
        "\n",
        "print(\"Best split criterion:\",best_tree.best_estimator_.get_params()['criterion'])\n",
        "\n",
        "print(\"Optimum Maximum Depth of Tree:\",best_tree.best_estimator_.get_params()['max_depth'])\n",
        "\n",
        "print(\"Accuracy for Decision Tree After Hyperparameter Tuning : \",metrics.accuracy_score(y_test,dt_prediction))\n",
        "\n",
        "\n",
        "# Feature Reduction\n",
        "# For feature reduction we are considering here two techniques: PCA,LDA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pca_knn = make_pipeline(StandardScaler(),PCA(n_components=2,random_state=0))\n",
        "pca_knn.fit(x_train)\n",
        "x_train_pca_scaled = pca_knn.transform(x_train)\n",
        "x_test_pca_scaled = pca_knn.transform(x_test)\n",
        "\n",
        "# knn\n",
        "\n",
        "knn.fit(x_train_pca_scaled,y_train)\n",
        "\n",
        "prediction_pca= knn.predict(x_test_pca_scaled)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix_main_pca = confusion_matrix(y_test,prediction_pca).ravel().tolist()\n",
        "print(confusion_matrix_main_pca)\n",
        "accuracy_pca= (confusion_matrix_main_pca[0]+confusion_matrix_main_pca[3]) / (confusion_matrix_main_pca[0]+confusion_matrix_main_pca[1]+confusion_matrix_main_pca[2]+confusion_matrix_main_pca[3])\n",
        "print(\"Accuracy for KNN after dimensionality reduction to 2 by PCA: \",accuracy_pca)\n",
        "\n",
        "# decision tree\n",
        "decisiontree.fit(x_train_pca_scaled,y_train)\n",
        "predict_dt_pca= decisiontree.predict(x_test_pca_scaled)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix_main_dt_pca = confusion_matrix(y_test,predict_dt_pca).ravel().tolist()\n",
        "print(confusion_matrix_main_dt_pca)\n",
        "accuracy_pca_dt= (confusion_matrix_main_dt_pca[0]+confusion_matrix_main_dt_pca[3]) / (confusion_matrix_main_dt_pca[0]+confusion_matrix_main_dt_pca[1]+confusion_matrix_main_dt_pca[2]+confusion_matrix_main_dt_pca[3])\n",
        "print(\"Accuracy for Decision Tree after dimensionality reduction to 2 by PCA: \",accuracy_pca_dt)\n",
        "\n",
        "\n",
        "# Feature Normalization\n",
        "\n",
        "#for feature normalization we can use standard scaler as well as min max scalar\n",
        "#here we are using min max scalar\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "min_max_scaler = MinMaxScaler()\n",
        "min_max_scaler.fit(x_train)\n",
        "x_train_val_min_max=min_max_scaler.transform(x_train)\n",
        "x_test_val_min_max=min_max_scaler.transform(x_test)\n",
        "\n",
        "# knn\n",
        "knn.fit(x_train_val_min_max,y_train)\n",
        "prediction_scaled = knn.predict(x_test_val_min_max)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix_main_minmax = confusion_matrix(y_test,prediction_scaled).ravel().tolist()\n",
        "print(confusion_matrix_main_minmax)\n",
        "accuracy_minmax= (confusion_matrix_main_minmax[0]+confusion_matrix_main_minmax[3]) / (confusion_matrix_main_minmax[0]+confusion_matrix_main_minmax[1]+confusion_matrix_main_minmax[2]+confusion_matrix_main_minmax[3])\n",
        "print(\"Accuracy for KNN after Min max Scaling: \",accuracy_minmax)\n",
        "\n",
        "\n",
        "\n",
        "# decision tree\n",
        "\n",
        "\n",
        "decisiontree.fit(x_train_val_min_max,y_train)\n",
        "prediction_scaled_dt = decisiontree.predict(x_test_val_min_max)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#confusion Matrix to check accuracy for decision tree fitted for data which scaled by using min max scaling\n",
        "confusion_matrix_main_minmax_dt = confusion_matrix(y_test,prediction_scaled_dt).ravel().tolist()\n",
        "print(confusion_matrix_main_minmax_dt)\n",
        "accuracy_minmax_dt= (confusion_matrix_main_minmax_dt[0]+confusion_matrix_main_minmax_dt[3]) / (confusion_matrix_main_minmax_dt[0]+confusion_matrix_main_minmax_dt[1]+confusion_matrix_main_minmax_dt[2]+confusion_matrix_main_minmax_dt[3])\n",
        "print(\"Accuracy for Decision Tree after Min max Scaling: \",accuracy_minmax_dt)\n",
        "\n"
      ],
      "metadata": {
        "id": "tvBZH6ilInsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37ee6c18-6780-4677-89b9-48e5382cbd00"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5        0.58823529 0.64705882 0.55882353 0.47058824 0.79411765\n",
            " 0.5        0.70588235 0.6969697  0.54545455]\n",
            "[0.93877551 0.95918367 1.         0.97916667 0.9375     0.95833333\n",
            " 0.97916667 1.         1.         0.9375    ]\n",
            "Mean Cross Validation Score For KNN with k folds:  0.6007130124777185\n",
            "Mean Cross Validation Score For Decision Tree with k folds:  0.9689625850340136\n",
            "Best value of K: 1\n",
            "Best Distance Metric: 1\n",
            "Accuracy for knn Tree After Hyperparameter Tuning :  0.7862068965517242\n",
            "Best split criterion: gini\n",
            "Optimum Maximum Depth of Tree: 6\n",
            "Accuracy for Decision Tree After Hyperparameter Tuning :  0.9724137931034482\n",
            "[52, 24, 25, 44]\n",
            "Accuracy for KNN after dimensionality reduction to 2 by PCA:  0.6620689655172414\n",
            "[55, 21, 21, 48]\n",
            "Accuracy for Decision Tree after dimensionality reduction to 2 by PCA:  0.7103448275862069\n",
            "[60, 16, 10, 59]\n",
            "Accuracy for KNN after Min max Scaling:  0.8206896551724138\n",
            "[75, 1, 2, 67]\n",
            "Accuracy for Decision Tree after Min max Scaling:  0.9793103448275862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New classifier (10 Marks)\n",
        "\n",
        "Replicate the previous task for a classifier that we did not cover in class. So different than K-NN and decision trees. Briefly describe your choice.\n",
        "Try to create the best model for the given dataset.\n",
        "Save your best model into your github. And create a single code cell that loads it and evaluate it on the following test dataset:\n",
        "https://github.com/andvise/DataAnalyticsDatasets/blob/main/dm_assignment2/sat_dataset_test.csv\n",
        "\n",
        "This link currently contains a sample of the training set. The real test set will be released after the submission. I should be able to run the code cell independently, load all the libraries you need as well."
      ],
      "metadata": {
        "id": "FYoMg0EZIrNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "#Base Code reference for my classifier Random Forest : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "#classifier Used Here is Gaussian Random Forest\n",
        "from pandas.io.formats.info import DataFrameInfo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "df = pd.read_csv(\"https://github.com/andvise/DataAnalyticsDatasets/blob/main/dm_assignment2/sat_dataset_test.csv?raw=true\")\n",
        "\n",
        "value = df.quantile(0.98)\n",
        "\n",
        "df=df.round(decimals = 2)\n",
        "#df = df.replace(np.inf, value)\n",
        "#df = df.replace(-np.inf, value)\n",
        "df.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "\n",
        "df.fillna(0,inplace=True)\n",
        "#df = df.drop(['saps_EstACL_Mean'], axis=1)\n",
        "\n",
        "df\n",
        "\n",
        "df.dtypes\n",
        "df\n",
        "\n",
        "df['target'].value_counts()\n",
        "\n",
        "# YOUR CODE HERE\n",
        "df1= df\n",
        "y= df['target']\n",
        "x = df1.iloc[: , :-1]\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.70,test_size=0.30,random_state=1)\n",
        "\n",
        "print(y_test)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest = RandomForestClassifier(max_depth=2,random_state=0)\n",
        "\n",
        "random_forest.fit(x_train,y_train)\n",
        "\n",
        "prediction_rf = random_forest.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix_rf = confusion_matrix(y_test, prediction_rf).ravel().tolist()\n",
        "print(confusion_matrix_rf)\n",
        "accuracy_rf= (confusion_matrix_rf[0]+confusion_matrix_rf[3]) / (confusion_matrix_rf[0]+confusion_matrix_rf[1]+confusion_matrix_rf[2]+confusion_matrix_rf[3])\n",
        "print(\"Accuracy for Random Forest : \",accuracy_rf)\n",
        "\n",
        "#Hold Out Cross Validation for random forest\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "rf_score = cross_val_score(random_forest,x_train,y_train,cv=10)\n",
        "print(\"Mean Cross Validation Score For Random Forest Classifier with k folds: \",rf_score.mean())\n",
        "\n",
        "#Hyperparameter tuning for random forest\n",
        "\n",
        "#here we are using the gridsearch cv method\n",
        "#lets create parameter grid for hyperparameter tuning\n",
        "\n",
        "hyperparameters = {\n",
        "    'bootstrap': [True],\n",
        "    'max_features' : ['auto', 'sqrt'],\n",
        "    'max_depth': [80, 90, 100, 110],\n",
        "    'min_samples_leaf': [3, 4, 5],\n",
        "    'min_samples_split': [8, 10, 12,14]\n",
        "}\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "gridcvrf= GridSearchCV(random_forest,hyperparameters,cv=10)\n",
        "best_forest = gridcvrf.fit(x_train,y_train)\n",
        "predictions_hyperparameters = gridcvrf.predict(x_test)\n",
        "\n",
        "print(\"Best parameters summary: \",gridcvrf.best_params_)\n",
        "print(\"Accuracy of Random Forest After Hyperparameter Tuning: \", metrics.accuracy_score(y_test,predictions_hyperparameters))\n",
        "\n",
        "#Feature Reduction Methods:\n",
        "#Here we are Using Principle Component Analysis\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pca_rf = make_pipeline(StandardScaler(),PCA(n_components=2,random_state=0))\n",
        "pca_rf.fit(x_train)\n",
        "x_train_pca_scaled = pca_rf.transform(x_train)\n",
        "x_test_pca_scaled = pca_rf.transform(x_test)\n",
        "\n",
        "#Now fit the random forest model with the data which is reduced in two dimensions\n",
        "random_forest.fit(x_train_pca_scaled,y_train)\n",
        "\n",
        "y_pred_pca= random_forest.predict(x_test_pca_scaled)\n",
        "\n",
        "print(\"Accuracy Score For PCA of Random Forest is: \",metrics.accuracy_score(y_test,y_pred_pca))\n",
        "\n",
        "#Feature Normalization Using Min Max Scalar\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "min_max_scaler = MinMaxScaler()\n",
        "min_max_scaler.fit(x_train)\n",
        "x_train_val_min_max=min_max_scaler.transform(x_train)\n",
        "x_test_val_min_max=min_max_scaler.transform(x_test)\n",
        "\n",
        "#Fit random forest with scaled data\n",
        "random_forest.fit(x_train_val_min_max,y_train)\n",
        "\n",
        "y_pred_min_max= random_forest.predict(x_test_val_min_max)\n",
        "\n",
        "print(\"Accuracy Score For Data scaled by min max scaling to perform Random Forest is: \",metrics.accuracy_score(y_test,y_pred_min_max))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QRJXrY2hI32F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6388d8c4-e082-46a0-8e7d-8cd3417d13df"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31     1\n",
            "246    1\n",
            "185    0\n",
            "110    1\n",
            "90     1\n",
            "      ..\n",
            "95     0\n",
            "122    1\n",
            "23     0\n",
            "13     0\n",
            "61     1\n",
            "Name: target, Length: 145, dtype: int64\n",
            "[76, 0, 5, 64]\n",
            "Accuracy for Random Forest :  0.9655172413793104\n",
            "Mean Cross Validation Score For Random Forest Classifier with k folds:  0.9762923351158646\n",
            "Best parameters summary:  {'bootstrap': True, 'max_depth': 80, 'max_features': 'auto', 'min_samples_leaf': 3, 'min_samples_split': 8}\n",
            "Accuracy of Random Forest After Hyperparameter Tuning:  0.9655172413793104\n",
            "Accuracy Score For PCA of Random Forest is:  0.6758620689655173\n",
            "Accuracy Score For Data scaled by min max scaling to perform Random Forest is:  0.9655172413793104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"blue\">FOR GRADING ONLY</font>\n",
        "\n",
        "Save your best model into your github. And create a single code cell that loads it and evaluate it on the following test dataset: \n",
        "https://github.com/andvise/DataAnalyticsDatasets/blob/main/dm_assignment2/sat_dataset_test.csv"
      ],
      "metadata": {
        "id": "Q01BjiiCJTR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump, load\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "# INSERT YOUR MODEL'S URL\n",
        "mLink = 'URL_OF_YOUR_MODEL_SAVED_IN_YOUR_GITHUB_REPOSITORY?raw=true'\n",
        "mfile = BytesIO(requests.get(mLink).content)\n",
        "model = load(mfile)\n",
        "# YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "IWx4lyuQI929"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w-k6H-1ZKTYo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}